from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch
import psycopg2, json
from urllib.parse import quote  # Elasticsearch ë¬¸ì„œ IDë¡œ URL ì•ˆì „í•˜ê²Œ ì¸ì½”ë”©í•˜ê¸° ìœ„í•¨
from dotenv import load_dotenv
from airflow.utils.log.logging_mixin import LoggingMixin
# ë‚˜ì¤‘ì— .env íŒŒì¼ ê²½ë¡œ ì„¤ì •í•´ì•¼í•¨
# load_dotenv(dotenv_path=Path("/opt/airflow/.env"))

    # conn = psycopg2.connect(
    #     host=os.getenv("DB_HOST"),
    #     database=os.getenv("DB_NAME"),
    #     user=os.getenv("DB_USERNAME"),
    #     password=os.getenv("DB_PASSWORD")
    # )

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "airflow",  # DAG ì†Œìœ ì
    "start_date": datetime(2024, 1, 1),  # DAG ì‹œì‘ ë‚ ì§œ
    "retries": 1,  # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
    "retry_delay": timedelta(minutes=5),  # ì¬ì‹œë„ ê°„ê²©
}
logger = LoggingMixin().log

# PostgreSQL â†’ Elasticsearch ë™ê¸°í™” í•¨ìˆ˜
def sync_to_es():
    # PostgreSQL ì—°ê²°
    conn = psycopg2.connect(
        # host="127.0.0.1",
        # host = "172.22.176.175",
        host = "host.docker.internal",
        database="news", 
        user="ssafyuser", 
        password="ssafy"
    )
    
    cursor = conn.cursor()

    # ìµœê·¼ 5ë¶„ ë‚´ ë³€ê²½ëœ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ
    cursor.execute("""
        SELECT title, writer, write_date, category, content, url, keywords, updated_at
        FROM news_article
        WHERE updated_at > now() - interval '5 minutes'
    """)
    rows = cursor.fetchall()
    if not rows:
        logger.info("ğŸ” ë³€ê²½ëœ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŒ (5ë¶„ ë‚´).")
    else:
        logger.info(f"ğŸ“° {len(rows)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")

    # Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    # es = Elasticsearch("http://localhost:9200") -> ë¡œì»¬ 
    # es = Elasticsearch("http://es01:9200") -> ê°™ì€ ë„ì»¤ë‚´ì— ìˆìœ¼ë©´ 
    es = Elasticsearch("http://172.18.0.5:9200") #-> ë‚´ ipì£¼ë©´ì„œ esí˜¸ì¶œ
    count = 0
    for row in rows:
        # URLì„ ì•ˆì „í•˜ê²Œ ì¸ì½”ë”©í•˜ì—¬ ES ë¬¸ì„œ IDë¡œ ì‚¬ìš©
        doc_id = quote(row[5], safe='')

        # ë¬¸ì„œ êµ¬ì¡° ì •ì˜
        doc = {
            "title": row[0],
            "writer": row[1],
            "write_date": row[2].isoformat(),
            "category": row[3],
            "content": row[4],
            "url": row[5],
            "keywords": row[6],
            "updated_at": row[7].isoformat()
        }

        # Elasticsearchì— ë¬¸ì„œ ì €ì¥
            # ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not es.exists(index="news", id=doc_id):
            # ì—†ìœ¼ë©´ ì €ì¥
            es.index(index="news", id=doc_id, document=doc)
            count += 1
            print("ì—†ëŠ” ë°ì´í„° ì €ì¥")
        else:
            # ì¡´ì¬í•˜ë©´ ë¬´ì‹œ (or log)
            print(f"ğŸ” ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ: {doc_id}")
            logger.info(f"âš ï¸ ì´ë¯¸ ì¡´ì¬: {doc_id}")
    logger.info(f"âœ… {count}ê°œ ë¬¸ì„œ Elasticsearchì— ì €ì¥ ì™„ë£Œ")
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cursor.close()
    conn.close()

# DAG ì •ì˜
with DAG(
    dag_id="sync_pg_to_es",  # DAG ì´ë¦„
    default_args=default_args,  # ê¸°ë³¸ ì¸ì
    schedule_interval="*/5 * * * *",  # ë§¤ 5ë¶„ë§ˆë‹¤ ì‹¤í–‰ (cron í˜•ì‹)
    catchup=False,  # ì´ì „ ì‹¤í–‰ ëˆ„ë½ë¶„ ì‹¤í–‰ ì•ˆ í•¨
) as dag:
    # Python í•¨ìˆ˜ task ì •ì˜
    sync_task = PythonOperator(
        task_id="sync_postgres_to_elasticsearch",  # task ì´ë¦„
        python_callable=sync_to_es,  # ì‹¤í–‰í•  í•¨ìˆ˜
    )

