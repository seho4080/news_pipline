version: "3.9"

x-env: &env
  AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
  AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
  AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
  AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
  AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Seoul
  AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"

x-vols: &vols
  - ./dags:/opt/airflow/dags:ro
  - ./plugins:/opt/airflow/plugins:ro
  - ./logs:/opt/airflow/logs
  - ./data:/opt/airflow/data

x-svc: &svc
  build: { context: ., dockerfile: Dockerfile.airflow }
  environment: *env
  user: "${AIRFLOW_UID:-50000}:0"
  volumes: *vols
  depends_on:
    postgres: { condition: service_healthy }
    redis:    { condition: service_healthy }
  restart: unless-stopped
  networks: [airflow]

services:
  postgres:
    image: postgres:13
    environment: { POSTGRES_USER: airflow, POSTGRES_PASSWORD: airflow, POSTGRES_DB: airflow }
    volumes: [ "pgdata:/var/lib/postgresql/data" ]
    healthcheck: { test: ["CMD","pg_isready","-U","airflow"], interval: 10s, timeout: 5s, retries: 5 }
    ports: ["5433:5432"]
    restart: unless-stopped
    networks: [airflow]

  redis:
    image: redis:7.2-bookworm
    expose: ["6379"]
    healthcheck: { test: ["CMD","redis-cli","ping"], interval: 10s, timeout: 3s, retries: 50, start_period: 30s }
    restart: unless-stopped
    networks: [airflow]

  airflow-init:
    <<: *svc
    user: "0:0"
    entrypoint: ["/bin/bash","-c"]
    command: >
      "mkdir -p /sources/{logs,dags,plugins} &&
       chown -R ${AIRFLOW_UID:-50000}:0 /sources &&
       exec /entrypoint airflow version"
    environment:
      <<: *env
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ""
    volumes: [ ".:/sources" ]

  airflow-webserver:
    <<: *svc
    command: webserver
    ports: ["8080:8080"]
    depends_on: { airflow-init: { condition: service_completed_successfully } }

  airflow-scheduler:
    <<: *svc
    command: scheduler
    depends_on: { airflow-init: { condition: service_completed_successfully } }

  airflow-worker:
    <<: *svc
    command: celery worker
    environment: { <<: *env, DUMB_INIT_SETSID: "0" }
    depends_on: { airflow-init: { condition: service_completed_successfully } }

  spark-master:
    build: { context: ., dockerfile: Dockerfile.spark }
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
    ports: ["8083:8080","7077:7077"]
    networks: [airflow]
    restart: unless-stopped

  spark-worker:
    build: { context: ., dockerfile: Dockerfile.spark }
    environment: { SPARK_MODE: worker, SPARK_MASTER_URL: spark://spark-master:7077 }
    depends_on: [spark-master]
    ports: ["8084:8081"]
    networks: [airflow]
    restart: unless-stopped

networks: { airflow: {} }
volumes:   { pgdata: {} }
