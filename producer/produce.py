
import os
import json
import time
import logging
import feedparser
import urllib.request
import ssl
import requests
from bs4 import BeautifulSoup
import re
from kafka import KafkaProducer
from dotenv import load_dotenv
from datetime import datetime
from urllib.parse import quote

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('producer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

context = ssl._create_unverified_context()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
KAFKA_BROKER = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
KAFKA_TOPIC = os.getenv("KAFKA_TOPIC", "news-raw")
RSS_FETCH_INTERVAL = int(os.getenv("RSS_FETCH_INTERVAL", "300"))  # 5ë¶„
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "30"))

CATEGORY_RSS = {
    "ìµœì‹ ": "https://www.yna.co.kr/rss/news.xml",
    "ì •ì¹˜": "https://www.yna.co.kr/rss/politics.xml", 
    "ë¶í•œ": "https://www.yna.co.kr/rss/northkorea.xml",
    "ê²½ì œ": "https://www.yna.co.kr/rss/economy.xml",
    "ë§ˆì¼“+": "https://www.yna.co.kr/rss/market.xml",
    "ì‚°ì—…": "https://www.yna.co.kr/rss/industry.xml",
    "ì‚¬íšŒ": "https://www.yna.co.kr/rss/society.xml",
    "ì „êµ­": "https://www.yna.co.kr/rss/local.xml",
    "ì„¸ê³„": "https://www.yna.co.kr/rss/international.xml",
    "ë¬¸í™”": "https://www.yna.co.kr/rss/culture.xml",
    "ê±´ê°•": "https://www.yna.co.kr/rss/health.xml",
    "ì—°ì˜ˆ": "https://www.yna.co.kr/rss/entertainment.xml",
    "ìŠ¤í¬ì¸ ": "https://www.yna.co.kr/rss/sports.xml",
    "ì˜¤í”¼ë‹ˆì–¸": "https://www.yna.co.kr/rss/opinion.xml",
    "ì‚¬ëŒë“¤": "https://www.yna.co.kr/rss/people.xml"
}


# RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
def fetch_rss(rss_url, retries=MAX_RETRIES):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    for attempt in range(retries):
        try:
            req = urllib.request.Request(
                rss_url,
                headers={'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'}
            )
            with urllib.request.urlopen(req, context=context, timeout=REQUEST_TIMEOUT) as response:
                data = response.read()
            
            feed = feedparser.parse(data)
            if feed.bozo:
                logger.warning(f"RSS íŒŒì‹± ê²½ê³ : {rss_url} - {feed.bozo_exception}")
            
            logger.info(f"RSS í”¼ë“œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´: {rss_url} ({len(feed.entries)}ê°œ ê¸°ì‚¬)")
            return feed
            
        except Exception as e:
            logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{retries}): {rss_url} - {e}")
            if attempt < retries - 1:
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
            else:
                logger.error(f"RSS í”¼ë“œ ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                return None



# ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
def crawl_article(url, retries=MAX_RETRIES):
    """ê¸°ì‚¬ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    for attempt in range(retries):
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'}
            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            origin = soup.select_one('.story-news')
            
            if not origin:
                logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
                return "ë³¸ë¬¸ ì—†ìŒ"

            content = ""
            # ë¶€ì œëª© ì¶”ì¶œ
            sub_titles = origin.find_all('h2')
            for tag in sub_titles:
                content += tag.get_text(strip=True) + " "
            
            # ë³¸ë¬¸ ë‹¨ë½ ì¶”ì¶œ
            paragraphs = origin.find_all('p')
            for tag in paragraphs:
                if not tag.attrs:  # ì†ì„±ì´ ì—†ëŠ” ìˆœìˆ˜ p íƒœê·¸ë§Œ
                    text = tag.get_text(strip=True)
                    if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                        content += text + " "

            # ì •ì œëœ ë‚´ìš© ë°˜í™˜
            cleaned_content = re.sub(r'\s+', ' ', content).strip()
            return cleaned_content if len(cleaned_content) > 50 else "ë³¸ë¬¸ ì—†ìŒ"
            
        except requests.RequestException as e:
            logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{retries}): {url} - {e}")
            if attempt < retries - 1:
                time.sleep(1 * (attempt + 1))  # ì ì§„ì  ë°±ì˜¤í”„
            else:
                logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨: {url}")
                return "ë³¸ë¬¸ ì—†ìŒ"
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {url} - {e}")
            return "ë³¸ë¬¸ ì—†ìŒ"


# ê¸°ì‚¬ ë°ì´í„° êµ¬ì¡°í™”
def enrich_article(entry, category="ì¼ë°˜"):
    """RSS entryë¥¼ í’ë¶€í•œ ê¸°ì‚¬ ë°ì´í„°ë¡œ ë³€í™˜"""
    try:
        content = crawl_article(entry.link)
        
        # ë‚ ì§œ ì •ê·œí™”
        published_date = entry.published if hasattr(entry, 'published') else datetime.now().isoformat()
        
        # ë°ì´í„° êµ¬ì¡°í™”
        article_data = {
            'title': entry.title.strip() if hasattr(entry, 'title') else "ì œëª© ì—†ìŒ",
            'write_date': published_date,
            'content': content,
            'url': entry.link,
            'category': category,
            'summary': entry.summary if hasattr(entry, 'summary') else "",
            'author': entry.author if hasattr(entry, 'author') else "ì—°í•©ë‰´ìŠ¤",
            'collected_at': datetime.now().isoformat(),
            'source': "ì—°í•©ë‰´ìŠ¤"
        }
        
        logger.debug(f"ê¸°ì‚¬ ë°ì´í„° êµ¬ì¡°í™” ì™„ë£Œ: {article_data['title'][:50]}...")
        return article_data
        
    except Exception as e:
        logger.error(f"ê¸°ì‚¬ ë°ì´í„° êµ¬ì¡°í™” ì‹¤íŒ¨: {entry.link if hasattr(entry, 'link') else 'unknown'} - {e}")
        return None


# Kafka Producer ì„¤ì • ë° ì´ˆê¸°í™”
def create_kafka_producer():
    """Kafka Producerë¥¼ ìƒì„±í•˜ê³  ì—°ê²°ì„ í™•ì¸"""
    try:
        producer = KafkaProducer(
            bootstrap_servers=KAFKA_BROKER,
            value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None,
            acks='all',  # ëª¨ë“  replicaì—ì„œ í™•ì¸
            retries=3,
            retry_backoff_ms=1000,
            request_timeout_ms=30000,
            max_in_flight_requests_per_connection=1,  # ìˆœì„œ ë³´ì¥
            compression_type='gzip'  # ì••ì¶•
        )
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        metadata = producer.bootstrap_connected()
        logger.info(f"Kafka Producer ì´ˆê¸°í™” ì„±ê³µ: {KAFKA_BROKER}")
        return producer
        
    except Exception as e:
        logger.error(f"Kafka Producer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise


# ë©”ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì „ì†¡ í•¨ìˆ˜
def collect_and_send_news():
    """ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  Kafkaë¡œ ì „ì†¡"""
    producer = create_kafka_producer()
    total_sent = 0
    total_failed = 0
    
    try:
        for category, url in CATEGORY_RSS.items():
            logger.info(f"ğŸ“° [{category}] ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
            
            # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
            feed = fetch_rss(url)
            if not feed or not feed.entries:
                logger.warning(f"RSS í”¼ë“œê°€ ë¹„ì–´ìˆìŒ: {category}")
                continue
            
            category_sent = 0
            category_failed = 0
            
            for entry in feed.entries:
                try:
                    # ê¸°ì‚¬ ë°ì´í„° êµ¬ì¡°í™”
                    article = enrich_article(entry, category)
                    if not article:
                        category_failed += 1
                        continue
                    
                    # Kafkaë¡œ ì „ì†¡
                    key = quote(article['url'])  # URLì„ í‚¤ë¡œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€)
                    future = producer.send(KAFKA_TOPIC, key=key, value=article)
                    
                    # ì „ì†¡ ê²°ê³¼ í™•ì¸ (ë¹„ë™ê¸°)
                    record_metadata = future.get(timeout=10)
                    logger.info(f"âœ… ì „ì†¡ ì„±ê³µ: {article['title'][:50]}... "
                              f"(í† í”½: {record_metadata.topic}, íŒŒí‹°ì…˜: {record_metadata.partition})")
                    
                    category_sent += 1
                    total_sent += 1
                    
                except Exception as e:
                    logger.error(f"âŒ ê¸°ì‚¬ ì „ì†¡ ì‹¤íŒ¨: {entry.title if hasattr(entry, 'title') else 'unknown'} - {e}")
                    category_failed += 1
                    total_failed += 1
            
            logger.info(f"ğŸ“Š [{category}] ì™„ë£Œ - ì„±ê³µ: {category_sent}, ì‹¤íŒ¨: {category_failed}")
            time.sleep(1)  # ì¹´í…Œê³ ë¦¬ ê°„ ê°„ê²©
        
        # ì „ì²´ ê²°ê³¼ ë¡œê¹…
        logger.info(f"ğŸ¯ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ - ì´ ì„±ê³µ: {total_sent}, ì´ ì‹¤íŒ¨: {total_failed}")
        
    finally:
        # Producer ì¢…ë£Œ
        producer.flush()
        producer.close()
        logger.info("Kafka Producer ì¢…ë£Œë¨")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ë‰´ìŠ¤ Producer ì‹œì‘")
    logger.info(f"ì„¤ì •: Kafka={KAFKA_BROKER}, Topic={KAFKA_TOPIC}, Interval={RSS_FETCH_INTERVAL}s")
    
    while True:
        try:
            start_time = time.time()
            collect_and_send_news()
            elapsed_time = time.time() - start_time
            
            logger.info(f"â±ï¸ ìˆ˜ì§‘ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ˜´ ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ {RSS_FETCH_INTERVAL}ì´ˆ ëŒ€ê¸°...")
            
            time.sleep(RSS_FETCH_INTERVAL)
            
        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            break
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            logger.info(f"60ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(60)

if __name__ == "__main__":
    main()
